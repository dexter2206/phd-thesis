\chapter{Solving spin-glass problems using tensor networks}
\label{chapter:tn}

Benchmarking quantum annealers requires more sophisticated algorithms. While
there exists a plethora of general-purpose optimization algorithms, one might
hope to achieve better results by exploiting the topology of the problem's
underlying graph and thus locality therein. In this chapter, we describe a
recent, tensor network-based algorithm for finding low-energy spectrum of Ising
spin-glasses, designed for problems defined on Chimera-like
quasi-two-dimensional graphs. The algorithm exploits the sparsity and locality
of the Chimera graph by representing the Boltzmann distribution of spin-glass
as a tensor network, whose approximate contraction can be used for computing
marginal probability distributions. This procedure can then be combined with
the well-known branch and bound algorithm to iteratively select the most
promising partial solutions, finally producing an approximation of the
low-energy spectrum.

\section{Introduction to tensor networks}

\todo[inline,color=SkyBlue]{Mention that the algorithm is physics inspired}
\todo[inline,color=SkyBlue]{Mention the other Chimera-specific algorithm}

\subsection{Branch and bound}
Let us start by considering an Ising spin glass problem defined on a square
lattice, as depicted in Fig. \ref{fig:lattice-and-border}. The state space of
such a system can be viewed as a tree, in which $k$-th level contains all
partial configurations $(s_1, \ldots, s_k)$. This representation allows one to
explore the state space incrementally in search for low energy states, and
possibly prune the less promising branches. In the approach described here, we
use marginal probability $p(s_1, s_2, \ldots, s_k)$ as a criterion for deciding
which partial configurations are most promising. More precisely, we explore the
solution tree in a top-down manner, keeping at most $M$ states at $k$-th level
and branching them into $2M$ new partial configurations at level $k+1$ . The
new marginal probability distributions can be computed as

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/squarelattice.pdf}
  \caption{\textbf{a.} An example Ising spin--glass  of 16 spins on a square lattice. The conditional probability for spins in the region $\overline{X}$ conditioned with a given configuration of spins in the region $X$ depends only on part of the configuration on the border $\partial X$. \textbf{b.} A fragment of state space tree. States kept at each level
    are marked with green, and the pruned branches are marked with red.}
  \label{fig:lattice-and-border}
\end{figure}

\begin{equation}
  \label{eq:conditional-prob}
  p(s_1, s_2, \ldots, s_k, s_{k+1}) = p(s_1, s_2, \ldots, s_k)p(s_{k+1}|, s_1, \ldots, s_k)
\end{equation}
We can exploit locality of the problem by observing that conditional
probability in \eqref{eq:conditional-prob} of configuration in the region $X =
  (1, 2, \ldots, k)$ depends only on configuration on the border $\partial X$ consisting of those spins, that directly interact with the region
$\overline{X} = (k+1, 2, \ldots N)$. To see that, denote by $H_X$ the usual
    Hamiltonian $H$ restricted to the graph induced by vertices in $X$. Further,
    let $H_{X, \overline{X}} = H - H_X - H_{\overline{X}}$. Notice that $H_{X,
    \overline{X}}$ contains only quadratic terms $J_{ij} s_i s_j$ such that $i \in
  X$ and $j \in \overline{X}$. Slightly abusing the notation, one may thus write
    \begin{equation}
      H(s_1, \ldots, s_N) = H_X(s_1, \ldots, s_k) + H_{\overline{X}}(s_{k+1}, \ldots, s_N) + H_{X, \overline{X}}(s_1, \ldots, s_N)
    \end{equation}
    Using definition of conditional probability applied to Boltzmann distribution,
    one thus gets
    \begin{align}
      p(s_{k+1}|s_1, \ldots, s_k) & = \frac{\sum\limits_{(z_{k+2}, \ldots, z_N)}e^{-\beta H(s_1, \ldots, s_{k+1}, z_{k+2},\ldots,z_N)}}{\sum\limits_{(z_{k+1}, \ldots, z_N)}e^{-\beta H(s_1, \ldots, s_k, z_{k+1},\ldots,z_N)}}                                                                                                                                                     \\
                                  & = \frac{\sum\limits_{(z_{k+2}, \ldots, z_N)}e^{-\beta (H_X(s_1, \ldots, s_k) + H_{\overline{X}}(s_{k+1}, z_{k+2},\ldots,z_N) + H_{X, \overline{X}}(s_1, \ldots, z_N))}}{\sum\limits_{(z_{k+1}, \ldots, z_N)}e^{-\beta (H_X(s_1, \ldots, s_k) + H_{\overline{X}}(z_{k+1}, \ldots,z_N) + H_{X, \overline{X}}(s_1, \ldots, z_N))}}                 \\
                                  & = \frac{e^{-\beta H_X(s_1, \ldots, s_k)}\sum\limits_{(z_{k+2}, \ldots, z_N)} e^{-\beta(H_{\overline{X}}(s_{k+1}, z_{k+2},\ldots,z_N) + H_{X, \overline{X}}(s_1, \ldots, z_N))}}{e^{-\beta H_X(s_1, \ldots, s_k)}\sum\limits_{(z_{k+1}, \ldots, z_N)}e^{ -\beta(H_{\overline{X}}(z_{k+1}, \ldots,z_N) + H_{X, \overline{X}}(s_1, \ldots, z_N))}} \\
                                  & = \frac{\sum\limits_{(z_{k+2}, \ldots, z_N)} e^{-\beta(H_{\overline{X}}(s_{k+1}, z_{k+2},\ldots,z_N) + H_{X, \overline{X}}(s_1, \ldots, z_N))}}{\sum\limits_{(z_{k+1}, \ldots, z_N)}e^{ -\beta(H_{\overline{X}}(z_{k+1}, \ldots,z_N) + H_{X, \overline{X}}(s_1, \ldots, z_N))}}
    \end{align}
    Note, in both numerator and denominator spins with indices from $X$ appear
    non-trivially only in $H_{X, \overline{X}}$ , i.e. the whole expression depends
    only on those spins in $X$ that directly interact with spins in $\overline{X}$,
which was to be demonstrated.

Before discussing how probabilities in \eqref{eq:conditional-prob} can be
computed, let us first extend the above approach to the more general case of a
quasi-two-dimensional graph, i.e. one in which nodes can be grouped into
\emph{clusters} forming a two-dimensional square lattice (see Fig.
\ref{fig:clustering} for details). One can easily see, that again we can
construct a tree-like structure representing state space, this time considering
joint configurations of spins in a single cluster. Therefore, for the most of
the time, we might "forget" the underlying spin-glass structure and consider
square lattices in which spin clusters act like higher-dimensional systems.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/clustering}
  \caption{Grouping spins into clusters in a Chimera-like graph.} \label{fig:clustering}
\end{figure}

A central object considered in our algorithm is the probability distribution
$\exp[-\beta H(\mathbf{s})]$, which we approximate using a PEPS-equivalent
tensor network, whose detail will be described shortly. Contracting such a
network can give the probability distribution of any full configuration, as
well as the marginal probabilities.

\begin{equation}
  p(s_1, s_2, \ldots, s_k) \sim \tr[\mathcal{P}_{(s_1, \ldots, s_k)} e^{-\beta H(\mathbf{s})}]
\end{equation}

\section{Grouping spins into clusters}
The outlined approach can be easily extended to the case of the problem defined
on a graph in which nodes can be grouped into \emph{clusters} forming a
two-dimensional lattice. See Fig. \ref{fig:clustering} for a detailed description.
In this more general case, $s_i$ in \eqref{eq:conditional-prob} denote join
probability of configuration in $i$-th cluster, and each configuration is
branched into $2^lM$ new ones, where $l$ is the number of spins in respective
cluster.

\section{PEPS network construction}
We begin construction of a PEPS network for a quasi-two-dimensional graph by
considering two spins at sites $i$ and $j$ connected by an edge $J_{ij}$. This
edge can be decomposed as

\begin{equation}
  e^{-\beta J_{ij}s_i s_j} = \sum_{\gamma = \pm 1} B^{s_{i\phantom{j}}}_\gamma C^{s_j}_\gamma
\end{equation}
where
\begin{equation}
  \label{eq:decomposition}
  B^{S_i}_\gamma = \delta_{\gamma s_i} \quad C^{s_j}_\gamma = e^{-\beta \gamma J_{ij} s_j}
\end{equation}
Note that decomposition \eqref{eq:decomposition}, although not unique, has the
advantage of comprising only non-negative coefficients, which positively
affects numerical stability. Next, with each cluster we associate a PEPS tensor
\begin{equation}
  \label{eq:peps}
  A^{\mathbf{s_c}}_{\mathbf{lrud}} = e^{-\beta H(\mathbf{s_c})} B^{\mathbf{s_c}^l}_\mathbf{l}C^{\mathbf{s_c}^r}_\mathbf{r}B^{\mathbf{s_c}^u}_\mathbf{u}C^{\mathbf{s_c}^d}_\mathbf{d}
\end{equation}
Here, $\mathbf{s_c}$ collects all spins in a given cluster, and
  $\mathbf{s_c}^l$, $\mathbf{s_c}^r$, $\mathbf{s_c}^u$, $\mathbf{s_c}^d$ collect
    spins interacting with it from the left, right, up and down respectively. Each
    such tensor has five legs: the physical one $\mathbf{s_c}$ of dimension $2^m$,
    where $m$ denotes the number of spins in the cluster, and the virtual ones $l,
  r, u, d$ with dimensions depending on the number of inter-cluster edges. Note
    that $H$ in \eqref{eq:peps} is restricted to the graph induced by spins
belonging to the considered cluster. The construction is depicted in Fig.
\ref{fig:tensors}.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/peps.pdf}
  \caption{Construction of PEPS network} \label{fig:tensors}
\end{figure}

\section{Results}

To fully investigate the performance of our algorithm, we performed several
benchmarks, testing various metrics quantifying both execution time, as well as
the quality of the found solutions. We tested our algorithm for sets of
\emph{droplet} instances specifically designed to be hard for classical
heuristic solvers, especially ones relying on local updates. We benchmarked our
algorithm against classical solvers based on Parallel-Tempering, and D-Wave
Quantum annealer DW-2000Q$_6$. As it is hard to directly compare samples
obtained from the D-Wave annealer with the output of our deterministic
algorithm, we decided to use time-to-solution as a metric. The time to solution
$\mbox{TTS}$ is defined as
    \begin{equation}
      \label{eq:tts}
      \mbox{TTS} = \tau \frac{\log(1 - p_{target})}{\log(1 - p_{succ})},
    \end{equation}
    where $p_{target}$ is the desired probability of obtaining solution, $p_{succ}$
    is the empirical probability of obtaining solution and $\tau$ is the running
    time of the solver. In addition, for D-Wave annealers we multiply $\mbox{TTS}$
    by the ratio $N/\mbox{num\_qubits}$, to account for the possibility of fitting
    multiple instances of the problem on the device at the same time. Naturally,
    one might consider $\mbox{TTS}$ metric not only for finding a ground state, but
also for finding a solution approximating a ground state with a given
approximation ratio ground state (i.e. solution lying in the desired lowest
fraction of the full energy spectrum). The results of these benchmarks are
presented in Table \ref{tab:tnvspt}. For all instances, our algorithm was able
to find the ground state, which was not the case for other solvers. However, if
one is not necessarily interested in finding the ground state, both D-Wave
annealers and classical Parallel Tempering solver might find a satisfying
solution in a shorter time.
\begin{table}[b]
  \centering
  \begin{tabular}{|l|c|ccc|}
    \hline
    \rowcolor{theader}  Method & approx. ratio & $N=512$   & $N=1152$  & $N=2048$  \\
    \hline
    TN                         & g.s.          & 30s       & 150s      & 450s      \\
    \hline
    \hline
    PT (adaptive)              & g.s.          & 800s      & ---       & ---       \\
    \hline
    PT (geometric)             & $0.01$        & 0.53s     & 4.16s     & ---       \\
    PT (geometric)             & $0.005$       & 2.51s     & 56.4s     & ---       \\
    PT (geometric)             & $0.001$       & 158.4s    & timed-out & ---       \\
    PT (geometric)             & $0.0001$      & 897.6s    & timed-out & ---       \\
    \hline
    \hline
    DWave 2000Q$_6$            & $0.01$        & $0.003$s  & $0.006$s  & $0.02$s   \\
    DWave 2000Q$_6$            & $0.005$       & $0.2$s    & timed-out & timed-out \\
    DWave 2000Q$_6$            & $0.001$       & timed-out & timed-out & timed-out \\
    \hline
    \hline
  \end{tabular}
  \caption{Comparison of time-to-solution metric for our tensor network based algorithm,
    in-house parallel tempering implementation and D-Wave 2000Q$_{6}$. The
    \emph{adaptive} and \emph{geometric} terms refer to the distribution of inverse
    temperature in Parallel Tempering replicas. Here, we bounded running time of
    our solver to 30 minutes with bond dimension $\chi = 16$, $\beta=3$ and
    probability cutoff $\delta_{p} = 10^{-3}$. For PT, the $\tau$ in equation
    \ref{eq:tts} is inferred from the running time and number of performed MC
    sweeps. In particular, a single MC sweep took 0.00005s for N=512 and 0.00011s
    for $N=1024$. For the adaptive PT, we used 12 replicas. For geometric PT, we
    used 25 replicas with geometrically distributed $\beta$, with
    $\beta_{\min}=0.0001$ and $\beta_{\max}=10$. For all probabilistic samplers, we
        used target probability $p_{\mbox{target}}=0.99$. In the case of D-Wave
        annealers, we modified instances by dropping inactive qubits. To obtain the
        reference ground state, we once again used our algorithm. We optimized time to
        solution over annealing times of $5\mu s$, $20\mu s$ and $200\mu s$. As the
        annealer running time we used only annealing time, i.e. we do not include other
        factors like readout time, reset time, etc. For each instance and each
        annealing time, we gathered 1000 samples for $N=512$ and 2500 for other values
        of $N$. The ``timed-out'' string indicates that the given algorithm could not
        find solution within given approximation ratio (i.e. $p_{\mbox{succ}}=0$). }
  \label{tab:tnvspt}
\end{table}

\begin{figure}
  \includegraphics[width=\textwidth]{figures/tn-single-state.pdf}
  \caption{Example result of running our algorithm on a droplet instance with $N=2048$.
    \textbf{a.} Low energy spectrum found by a single run of our algorithm. Observe
    consistency between different values of $\beta$. \textbf{b.} Hamming distance
    of solutions presented in \textbf{a.} from the ground state. \textbf{c.}
    Probabilities of each configuration found for least numerically stable values
    of $\beta$. In the depicted example, we can see full consistency between the
    probabilities obtained from contracting PEPS network $p_{n}$ and the Boltzmann
    weights calculated from the configuration's energy. \textbf{d.} Comparison of
    largest discarded probability $p_{d}$ and the ground state probability $p_{1}$.
        With increasing $\beta$ we were able to achieve $p_{d} < p_{1}$, indicating
    that the algorithm indeed reached the ground state. }
  \label{fig:tn-single-state}
\end{figure}

\begin{figure}
  \includegraphics[width=\textwidth]{figures/tn-ground-degeneracy.pdf}
  \caption{Histogram of ground state degeneracy found by our algorithm fore test instances constructed by
    drawing couplings $J_{ij}$ uniformly from a set $\{\pm 1, \pm 2, \pm 4\}$ and
    setting all local fields $h_{i} = 0$.}
  \label{fig:ground-degeneracy}
\end{figure}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
