\chapter*{Appendix}


\section{Asymptotic notation}

In order to characterize complexity of algorithms, it is useful to use asymptotic big-O notation. Consider two functions $f, g \colon \NN \to \RR$. We say that $f$ is $O(g)$ if and only if there exists a constant $C > 0$ and a natural number $n_{C}$ such that the inequality $0 \le f(n) \le C\cdot g(n)$ holds for all $n > n_{C}$ \cite{clrs}. It is common to write $f=O(g)$ instead of ``$f$ is $O(g)$'', slightly abusing the mathematical notation \cite{clrs}. One should notice that big-O notation does not provide a tight bound. For instance, we have $n + 1 = O(n)$ (since $n + 1 \le 2 \cdot n$) but also $n+1 = O(n^{10})$.

In the context of computational complexity, big-O notation is most commonly used for expressing upper bound on number of (dominating) operations performed by an algorithm as a function of its input size $N$. Since the number of performed operations is roughly proportional to the algorithm's execution time, it follows that algorithms with better bound can be considered as more performant. However, care must be taken when applying this reasoning to judge practical performance. In particular, one should be mindful of the constant factor $C$ in the definition above, as well as any bottlenecks stemming from the working of the underlying hardware. As a concrete example, Strassen's algorithm for multiplying two $N \times N$ matrices requires $O(N^{\alpha})$ multiplications, where $2 < \alpha < 3$, and yet may perform  worse than naive algorithm peforming $N^{3}$ multiplications, even for $N$ of order of several hundreds \cite{dalberto}.

We conclude this section by mentioning that there exist several other asymptotic notations. For instance, $\Omega$, describing asymptotic lower bound of a function, and $\Theta$ combining big-O and $\Theta$. For more details we refer reader to \cite{clrs}.
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
